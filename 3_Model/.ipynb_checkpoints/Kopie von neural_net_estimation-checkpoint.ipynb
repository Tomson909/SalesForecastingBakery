{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Tomson909/SalesForecastingBakery/blob/main/3_Model/neural_net_estimation.ipynb","timestamp":1703062422744}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d-3CyESydUAq"},"source":["## Mounting Google Drive and Loading Data\n","\n","First, let's mount the Google Drive to access the preprocessed data. Ensure that your data files are stored in the correct path within your Google Drive."]},{"cell_type":"markdown","source":["# To be done:\n","- one hot encoding must be done\n","- more datapreparation i guess"],"metadata":{"id":"r8xHgk_CeYnw"}},{"cell_type":"code","source":["!rm /content/test_data.csv\n","!rm /content/train_data.csv\n","!wget wget https://raw.githubusercontent.com/Tomson909/SalesForecastingBakery/facd9dec9e220b7a4d6421fd335846e34e46297f/3_Model/test_data.csv\n","!wget https://raw.githubusercontent.com/Tomson909/SalesForecastingBakery/facd9dec9e220b7a4d6421fd335846e34e46297f/3_Model/train_data.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYiuTl23nhFn","outputId":"0f4c20f8-c410-470b-f19b-ae9c2bb44c10","executionInfo":{"status":"ok","timestamp":1703062467602,"user_tz":-60,"elapsed":1502,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/test_data.csv': No such file or directory\n","rm: cannot remove '/content/train_data.csv': No such file or directory\n","--2023-12-20 08:54:26--  http://wget/\n","Resolving wget (wget)... failed: Name or service not known.\n","wget: unable to resolve host address ‘wget’\n","--2023-12-20 08:54:26--  https://raw.githubusercontent.com/Tomson909/SalesForecastingBakery/facd9dec9e220b7a4d6421fd335846e34e46297f/3_Model/test_data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 133739 (131K) [text/plain]\n","Saving to: ‘test_data.csv’\n","\n","test_data.csv       100%[===================>] 130.60K  --.-KB/s    in 0.02s   \n","\n","2023-12-20 08:54:26 (5.94 MB/s) - ‘test_data.csv’ saved [133739/133739]\n","\n","FINISHED --2023-12-20 08:54:26--\n","Total wall clock time: 0.4s\n","Downloaded: 1 files, 131K in 0.02s (5.94 MB/s)\n","--2023-12-20 08:54:26--  https://raw.githubusercontent.com/Tomson909/SalesForecastingBakery/facd9dec9e220b7a4d6421fd335846e34e46297f/3_Model/train_data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 524905 (513K) [text/plain]\n","Saving to: ‘train_data.csv’\n","\n","train_data.csv      100%[===================>] 512.60K  --.-KB/s    in 0.04s   \n","\n","2023-12-20 08:54:27 (13.4 MB/s) - ‘train_data.csv’ saved [524905/524905]\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime\n","train_data = pd.read_csv('/content/train_data.csv')\n","validation_data = pd.read_csv('/content/test_data.csv')\n"],"metadata":{"id":"zW1yixRmpNCD","executionInfo":{"status":"ok","timestamp":1703062521698,"user_tz":-60,"elapsed":675,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The following contains datacleaning and preparation"],"metadata":{"id":"VuBowkgVb8u8"}},{"cell_type":"code","source":["from datetime import datetime\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Function to map weekdays to dates\n","def map_weekday(date_str):\n","    date_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n","    return date_object.strftime(\"%A\")  # %A returns the full weekday name (e.g., Monday)\n","\n","# Apply the mapping function to the 'Datum' column\n","train_data['Weekday'] = train_data['Datum'].apply(map_weekday)\n","validation_data['Weekday'] = train_data['Datum'].apply(map_weekday)\n","\n","# Use LabelEncoder to encode weekdays as integers\n","label_encoder = LabelEncoder()\n","train_data['Weekday_Encoded'] = label_encoder.fit_transform(train_data['Weekday'])\n","validation_data['Weekday_Encoded'] = label_encoder.fit_transform(validation_data['Weekday'])"],"metadata":{"id":"ruwNFZpUcOu0","executionInfo":{"status":"ok","timestamp":1703062625276,"user_tz":-60,"elapsed":1248,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Which columns do we want to use?"],"metadata":{"id":"DASFHmabcZMV"}},{"cell_type":"code","source":["import numpy as np\n","train_wochentage = np.array([datetime.strptime(i,'%Y-%m-%d').weekday() for i in train_data['Datum']])/7\n","\n","train_data_x = train_data.drop(['Umsatz','Datum','Wettercode', 'Produktname', 'Wettererscheinung', 'IsNiederschlag', 'Weekday'], axis =1)\n","train_data_y = train_data[['Umsatz']]\n","\n","test_data_x = validation_data.drop(['Umsatz','Datum','Wettercode', 'Produktname', 'Wettererscheinung', 'IsNiederschlag', 'Weekday'], axis =1)\n","test_data_y = validation_data[['Umsatz']]"],"metadata":{"id":"GZWDczNBcY-0","executionInfo":{"status":"ok","timestamp":1703062696492,"user_tz":-60,"elapsed":332,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["removing missing values, almolst linear regression"],"metadata":{"id":"3TXS1Uh8cqJu"}},{"cell_type":"code","source":["# remove nans\n","# Handling missing values\n","def fill_missing_value(dataset):\n","    missing = np.where(pd.isnull(dataset))\n","    # find previous which is existent\n","    for s in missing[1]:\n","        for i in missing[0]:\n","            top_variable = np.nan\n","            bot_variable = np.nan\n","            l = 1\n","            while pd.isna(top_variable):\n","                top_variable = dataset.iloc[i + l, s]\n","                l += 1\n","\n","            k = 1\n","            while pd.isna(bot_variable):\n","                bot_variable = dataset.iloc[i - k, s]\n","                k += 1\n","\n","            # Fill missing value with the mean of top and bottom variables\n","            dataset.iloc[i, s] = np.mean([top_variable, bot_variable])\n","\n","    return dataset\n","train_data_x_nonan = fill_missing_value(train_data_x)\n","test_data_x_nonan = fill_missing_value(test_data_x)"],"metadata":{"id":"JjLbMYMecs19","executionInfo":{"status":"ok","timestamp":1703062741214,"user_tz":-60,"elapsed":1590,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def normalize(df):\n","  for column in df.columns:\n","    if not 'Is' in column:\n","      df[column] = (df[column] - np.mean(df[column]))/np.std(df[column])\n","  return df\n","\n","train_data_y_norm = normalize(train_data_y)\n","validation_data_y_norm = normalize(test_data_y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnsLaLHWf7kL","executionInfo":{"status":"ok","timestamp":1703063636184,"user_tz":-60,"elapsed":297,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}},"outputId":"afc2d02e-19a8-41d2-d3fa-b46dd600b87b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-1ee1ad1f8c70>:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column] = (df[column] - np.mean(df[column]))/np.std(df[column])\n"]}]},{"cell_type":"markdown","metadata":{"id":"QEn1WecUkUvY"},"source":["## Defining the Neural Network\n","\n","Now, let's define our neural network. We are using a Sequential model definition from Keras with batch normalization and dense layers."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIVMJ2ISYuJ1","outputId":"fa9410db-5cff-477b-edcb-14693e339a77","executionInfo":{"status":"ok","timestamp":1703063653281,"user_tz":-60,"elapsed":342,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","\n","model = Sequential([\n","  InputLayer(input_shape=(len(train_data_x.columns), )),\n","  BatchNormalization(),\n","  Dense(100, activation='relu'),\n","  Dense(50, activation='relu'),\n","  Dense(1)\n","])\n","\n","model.summary()\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," batch_normalization_2 (Bat  (None, 8)                 32        \n"," chNormalization)                                                \n","                                                                 \n"," dense_6 (Dense)             (None, 100)               900       \n","                                                                 \n"," dense_7 (Dense)             (None, 50)                5050      \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 6033 (23.57 KB)\n","Trainable params: 6017 (23.50 KB)\n","Non-trainable params: 16 (64.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"GHLvD-uEkbJI"},"source":["## Compiling and Training the Model\n","\n","We will compile the model using Mean Squared Error (MSE) as the loss function and Adam optimizer. The model is then trained using the training data."]},{"cell_type":"code","metadata":{"id":"3C9DP_BFZBdd","colab":{"base_uri":"https://localhost:8080/","height":497},"outputId":"bc578904-3b90-4888-9629-c34a93a0cd0b","executionInfo":{"status":"error","timestamp":1703063844466,"user_tz":-60,"elapsed":3244,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"source":["model.compile(loss=\"mae\", optimizer=Adam(learning_rate=0.001), metrics='mae')\n","\n","history = model.fit(train_data_x_nonan, train_data_y_norm, epochs=200,\n","                    validation_data=(test_data_x_nonan, validation_data_y_norm), verbose = 1)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["235/235 [==============================] - 1s 4ms/step - loss: 0.3362 - mae: 0.3362 - val_loss: 0.3518 - val_mae: 0.3518\n","Epoch 109/200\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3255 - mae: 0.3255 - val_loss: 0.3459 - val_mae: 0.3459\n","Epoch 110/200\n","235/235 [==============================] - 2s 7ms/step - loss: 0.3470 - mae: 0.3470 - val_loss: 0.3458 - val_mae: 0.3458\n","Epoch 111/200\n"," 10/235 [>.............................] - ETA: 1s - loss: 0.3783 - mae: 0.3783"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-eefa9dfbbf10>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_data_x_nonan, train_data_y_norm, epochs=200,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     validation_data=(test_data_x_nonan, validation_data_y_norm), verbose = 1)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## Saving the Trained Model\n","\n","After training, it's a good practice to save the model for future use."],"metadata":{"id":"0S-qupIYn_7V"}},{"cell_type":"code","source":["model.save(\"python_model.h5\")"],"metadata":{"id":"gItgvuncoD_9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b50b026-9136-4ade-fa47-71c18418fb81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"markdown","source":["## Plotting Training History\n","\n","Visualizing the training and validation loss can help us understand the model's performance over time."],"metadata":{"id":"XeywUAjmoGP1"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Model Loss During Training')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"OJjar_X8oL4t","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"2f7286aa-b943-49f4-9751-408178646727","executionInfo":{"status":"error","timestamp":1703063146606,"user_tz":-60,"elapsed":324,"user":{"displayName":"Tom Ruge","userId":"05714351898972986287"}}},"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-848250dc8d28>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Loss During Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x600 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Making Predictions and Evaluating the Model\n","\n","Let's use the model to make predictions on our training and validation sets and evaluate the model's performance using Mean Absolute Percentage Error (MAPE)."],"metadata":{"id":"cQRMNHFOoJyO"}},{"cell_type":"code","source":["import numpy as np\n","\n","def mape(y_true, y_pred):\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    non_zero_mask = y_true != 0\n","    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n","\n","training_predictions = model.predict(training_features)\n","validation_predictions = model.predict(validation_features)\n","training_predictions\n","print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n","print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")\n"],"metadata":{"id":"xUyG8jG5oQxy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9710306-04bb-4e93-8b80-0a17db65c665"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["379/379 [==============================] - 1s 2ms/step\n","109/109 [==============================] - 0s 2ms/step\n","MAPE on the Training Data: 33.67%\n","MAPE on the Validation Data: 33.70%\n"]}]},{"cell_type":"markdown","source":["## Visualizing Predictions vs Actual Values\n","\n","Visualizing the predicted versus actual values can provide insights into the model's accuracy."],"metadata":{"id":"1pzsqOgeoWvT"}},{"cell_type":"code","source":["def plot_predictions(data, title):\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(data['actual'], label='Actual Values', color='red')\n","    plt.plot(data['prediction'], label='Predicted Values', color='blue')\n","    plt.title(title)\n","    plt.xlabel('Case Number')\n","    plt.ylabel('Price in 1.000 USD')\n","    plt.legend()\n","    plt.show()\n","\n","data_train = pd.DataFrame({'prediction': training_predictions.flatten(), 'actual': training_labels.flatten()})\n","data_validation = pd.DataFrame({'prediction': validation_predictions.flatten(), 'actual': validation_labels.flatten()})\n","\n","plot_predictions(data_train.head(100), 'Predicted and Actual Values for the Training Data')\n","plot_predictions(data_validation.head(100), 'Predicted and Actual Values for the Validation Data')\n"],"metadata":{"id":"0y6IKrzMoZr3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example of a Prediction and Export of Test Data for Kaggle\n","\n","Finally, for testing data on Kaggle you ave to do a prediction for the test data and format it in the format needed by Kaggle.\n","\n","The code below provides an example that you can"],"metadata":{"id":"x0kFqMRPod7t"}},{"cell_type":"code","source":["# Extract IDs and prepare test features\n","test_ids = test_features['id']\n","test_features_for_prediction = test_features.drop(columns=['id'])\n","\n","# Make predictions\n","test_predictions = model.predict(test_features_for_prediction)\n","\n","# Create a DataFrame for submission\n","predictions_df = pd.DataFrame({\n","    'id': test_ids,\n","    'Predicted_Value': test_predictions.flatten() # Replace column name with name given in the smaple_submission.csv\n","})\n","\n","# Export to CSV\n","predictions_df.to_csv('kaggle_submission.csv', index=False)\n","\n","print(\"Submission file created: 'kaggle_submission.csv'\")"],"metadata":{"id":"Ap08Yyz1ojCg"},"execution_count":null,"outputs":[]}]}